# Seed everything (LightningCLI handles this when set here)
seed_everything: 42

# ----------------------------
# Model hyperparameters
# ----------------------------
model:
  in_channels: 1            # set to 3 for RGB datasets later
  out_channels: 1
  z_dim: 16
  num_classes: 10
  cond_dim: 16
  use_film: false
  sigma: 0.1

  # Optimization + loss schedule
  lr: 0.002
  weight_decay: 0.0001
  beta: 1.0                 # final KL weight
  kl_warmup_epochs: 10      # linear warmup from 0 -> beta
  max_epochs: 50            # (used by the cosine LR helper)

# ----------------------------
# Data settings
# ----------------------------
data:
  data_dir: ./data
  batch_size: 128
  num_workers: 8
  val_split: 5000
  image_channels: 1         # keep in sync with model.{in,out}_channels
  num_classes: 10
  predict_samples_per_class: 8
  pin_memory: true
  persistent_workers: true

# ----------------------------
# Trainer configuration
# ----------------------------
trainer:
  default_root_dir: runs
  accelerator: auto
  devices: auto
  max_epochs: 50
  precision: 32-true
  log_every_n_steps: 50
  gradient_clip_val: 1.0
  deterministic: false
  enable_checkpointing: true
  default_root_dir: ./runs

  # # ----------------------------
  # # Logger (TensorBoard)
  # # ----------------------------
  # logger:
  #   class_path: lightning.pytorch.loggers.TensorBoardLogger
  #   init_args:
  #     save_dir: runs    # Placeholder: will be set automatically to <run_dir>/logs [callbacks/run_dir_bootstrap.py]
  #     name: logs
  #     default_hp_metric: false
  #
  # # ----------------------------
  # # Callbacks
  # # ----------------------------
  # callbacks:
  #   # # Update the output directories
  #   # - class_path: callbacks.run_dir_bootstrap.RunDirBootstrap
  #   #   init_args: 
  #   #     base_path: runs 
  #
  #   # Save the best checkpoint
  #   - class_path: lightning.pytorch.callbacks.ModelCheckpoint
  #     init_args:
  #       monitor: val/loss
  #       mode: min
  #       save_last: true
  #       save_top_k: 1
  #       dirpath: runs    # Placeholder: will be set automatically to <run_dir>/checkpoints/best [callbacks/run_dir_bootstrap.py]
  #       filename: "epoch={epoch}-loss={val/loss:.4f}"
  #   # Save every 5 epochs (regardless of score)
  #   - class_path: lightning.pytorch.callbacks.ModelCheckpoint
  #     init_args:
  #       every_n_epochs: 5
  #       save_top_k: -1
  #       dirpath: runs    # Placeholder: will be set automatically to <run_dir>/checkpoints/interval [callbacks/run_dir_bootstrap.py]
  #       filename: "epoch-{epoch}"
  #       save_last: false
  #
  #   # Log learning rate every epoch
  #   - class_path: lightning.pytorch.callbacks.LearningRateMonitor
  #     init_args:
  #       logging_interval: epoch
  #
  #   # Output 8 sample images every epoch in validation
  #   - class_path: callbacks.sample_images.SampleImages
  #     init_args: 
  #       num_per_class: 8 
  #
