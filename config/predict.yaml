seed_everything: 42

# ----------------------------
# Model hyperparameters
# ----------------------------
model:
  in_channels: 1            # set to 3 for RGB datasets later
  out_channels: 1
  z_dim: 16
  num_classes: 10
  cond_dim: 16
  use_film: false

  # Optimization + loss schedule
  lr: 0.002                 # TODO: Update the learning rate based on learning rate finder
  weight_decay: 0.0001
  beta: 1.0                 # final KL weight
  kl_warmup_epochs: 10      # linear warmup from 0 -> beta
  max_epochs: 50            # (used by the cosine LR helper)

# ----------------------------
# Data settings
# ----------------------------
data:
  data_dir: ./data
  batch_size: 128           # TODO: Update the batch size based on batch size finder
  num_workers: 8
  val_split: 5000
  image_channels: 1         # keep in sync with model.{in,out}_channels
  num_classes: 10
  pin_memory: true
  persistent_workers: true
  
  # Controlling the output
  predict_classes: [7]
  temperature: 0.1
  guidance_scale: 0
  cond_scale: 1.0
  predict_samples_per_class: 8

# ----------------------------
# Trainer configuration
# ----------------------------
trainer:
  default_root_dir: runs
  accelerator: auto
  devices: auto
  max_epochs: 50
  precision: 32-true
  log_every_n_steps: 50
  gradient_clip_val: 1.0
  deterministic: false
  enable_checkpointing: true
  default_root_dir: ./runs

