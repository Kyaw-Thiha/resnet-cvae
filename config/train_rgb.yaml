# Seed everything (LightningCLI handles this when set here)
seed_everything: 42

# ----------------------------
# Model hyperparameters
# ----------------------------
model:
  in_channels: 3            
  out_channels: 3
  z_dim: 128
  num_classes: 10
  cond_dim: 16
  use_film: false

  # Optimization + loss schedule
  lr: 3.0e-4                # TODO: Update the learning rate based on learning rate finder
  weight_decay: 1.0e-4
  beta: 0.25                # final KL weight [0.5 ~ 1]
  kl_warmup_epochs: 40      # linear warmup from 0 -> beta
  max_epochs: 80            # (used by the cosine LR helper)
  loss:
    name: standard

# ----------------------------
# Data settings
# ----------------------------
data:
  data_dir: ./data
  batch_size: 128           # TODO: Update the batch size based on batch size finder
  num_workers: 8
  val_split: 5000
  image_channels: 3         # keep in sync with model.{in,out}_channels
  num_classes: 10
  predict_samples_per_class: 8
  pin_memory: true
  persistent_workers: true

# ----------------------------
# Trainer configuration
# ----------------------------
trainer:
  default_root_dir: runs
  accelerator: auto
  devices: auto
  max_epochs: 80
  precision: 32-true
  log_every_n_steps: 50
  gradient_clip_val: 1.0
  deterministic: false
  enable_checkpointing: true
  default_root_dir: ./runs
